---
title: "4.5.17 The Long Exodus Gets Somewhere, Milestone 2"
author: "Craig Alder"
date: "4/5/2017"
output: html_document
---
I have worked really hard developing this code and learned that the internet R discussions extend deep into the internet -- and it finally works! The printout at the end of this document is a sample of what it pulls. I can apply this same form to the rest of the blogs. You can also see the other variables I have collected and the extensive preparation of html at [https://docs.google.com/spreadsheets/d/1zxiNYbXcBGu8K20ZpYdut8ihJFLYanl3i-tdSv0TagQ/edit#gid=0] (https://docs.google.com/spreadsheets/d/1zxiNYbXcBGu8K20ZpYdut8ihJFLYanl3i-tdSv0TagQ/edit#gid=0). My repository should have documentation of the extensive work it took to get this seemingly simple code. Next week I will run the topic modelling with the librarian I've been working with, so I can then plot the data.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(rvest)    
url <- "http://www.ashleyannphotography.com/blog/2008/05/31/the-making-of-a-wet-willy/"
#Select first page.

getPostContent <- function(url){
    Sys.sleep(2)
    #Introduces pauses to convince server not robot.
    read_html(url) %>% 
        html_nodes(".article-content")%>%           
        html_text() %>% 
        gsub(x = ., pattern = '[\a\t\n]', replacement = '')
  }

#Pulls node for post content.

getDate <- function(url) {
    Sys.sleep(2.6)
    read_html(url) %>% 
        html_node(".updated") %>%
        html_text()
   #     Sys.sleep(1:3)
}
#Pulls node for date.

getTitle <- function(url) {
    Sys.sleep(.8)
    read_html(url) %>% 
        html_node(".article-title") %>%
        html_text()
       # Sys.sleep(1:3)
    }
#Pulls node for title.

getNextUrl <- function(url) {
    Sys.sleep(.2)
    read_html(url) %>% 
        html_node(".prev-post-link-wrap a") %>%
        html_attr("href")
    }
#Pulls node for url to previous post.

scrapeBackMap <- function(url, n){
    Sys.sleep(3)
    purrr::map_df(1:n, ~{
        if(!is.na(url)){
        oUrl <- url
        date <- getDate(url)
        post <- getPostContent(url)
        title <- getTitle(url)
        url <<- getNextUrl(url)
           
        data.frame(curpage = oUrl, 
                        nexturl = url,
                        posttext = post,
                        pubdate = date,
                        ptitle = title
#prepares functions for dataframe
                        )}
    })
}
   hey <- scrapeBackMap(url, 2)
   class(hey)
   str(hey)
#creates dataframe
```
The link to my Github account is [https://github.com/craigalder](https://github.com/craigalder). The link to my repository for this project is [https://github.com/craigalder/NSRG741_AlderProject](https://github.com/craigalder/NSRG741_AlderProject).

Error: 'NA' does not exist in current working directory ('/Users/craigalder/Documents/Big Data Class/N741 Data Wrangling/NSRG741_AlderProject').
17.
stop("'", path, "' does not exist", if (!is_absolute_path(path)) paste0(" in current working directory ('", getwd(), "')"), ".", call. = FALSE)
16.
check_path(path)
15.
path_to_connection(x)
14.
read_xml.character(x, encoding = encoding, ..., as_html = TRUE, options = options)
13.
read_xml(x, encoding = encoding, ..., as_html = TRUE, options = options)
12.
withCallingHandlers(expr, warning = function(w) invokeRestart("muffleWarning"))
11.
suppressWarnings(read_xml(x, encoding = encoding, ..., as_html = TRUE, options = options))
10.
read_html.default(url)
9.
read_html(url)
8.
eval(expr, envir, enclos)
7.
eval(lhs, parent, parent)
6.
read_html(url) %>% html_node(".updated") %>% html_text()
5.
getDate(url)
4.
.f(.x[[i]], ...)
3.
map(.x, .f, ...)
2.
purrr::map_df(1:n, ~{ oUrl <- url date <- getDate(url) post <- getPostContent(url) ...
1.
scrapeBackMap(url, 3000)
